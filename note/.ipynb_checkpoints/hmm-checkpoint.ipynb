{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM基本算法\n",
    "\n",
    "### 概率计算\n",
    "前向，后向求观测序列概率,另外还有为模型估计的BW算法做准备的:\n",
    "\n",
    "* 前向概率$\\alpha(i)$.\n",
    "\n",
    "* 后向概率$\\beta(i)$.\n",
    "\n",
    "* 可以计算观测预测时刻t,某状态为qi的概率$\\gamma(i)$.\n",
    "\n",
    "* 可以计算观测预测时刻t,某状态为qi,时刻t+1,某状态为qj的联合概率$\\xi(i,j)$.\n",
    "\n",
    "* 根据观测预测某个状态在T内出现的期望值..\n",
    "\n",
    "### 状态序列预测\n",
    "维特比算法,动态规划算法\n",
    "\n",
    "### 学习方法估计模型\n",
    "BW算法,由EM算法而来. 又叫前向后向算法，因为需要用到上面提到的前向后向概率.\n",
    "\n",
    "由EM算法可知,分为e步和q步, 不断逼近原来的极大似然估计极值(局部极值).\n",
    "\n",
    "**e-step** 通过$\\bar{\\lambda}$更新Q,Q是关于$\\lambda=(A,B,\\pi)$的函数\n",
    "\n",
    "$Q(\\lambda,\\bar{\\lambda})=\\sum_{I}(O,I|\\lambda)P(O,I|\\bar{\\lambda})$\n",
    "\n",
    "**m-step** 对Q求极值，求得$\\theta^{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observe seqence probability cal(fp): (0.130218, array([[ 0.1     ,  0.077   ,  0.04187 ],\n",
      "       [ 0.16    ,  0.1104  ,  0.035512],\n",
      "       [ 0.28    ,  0.0606  ,  0.052836]]))\n",
      "observe seqence probability cal(bp): (0.130218, array([[ 0.2451,  0.54  ,  1.    ],\n",
      "       [ 0.2622,  0.49  ,  1.    ],\n",
      "       [ 0.2277,  0.57  ,  1.    ]]))\n",
      "observe seqence probability cal(total): (0.130218, 0.130218, 0.130218)\n",
      "veterbi cal: [2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# %load ../HMM.py\n",
    "# HMM.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# delta nx1\n",
    "# A nxn\n",
    "# return nx1\n",
    "def max_delta(delta,A):\n",
    "    [m,n]=A.shape;\n",
    "    vec=np.zeros([m,1]);\n",
    "    idx=np.zeros([m,1]);\n",
    "    for i in range(n):\n",
    "        vec[i] = np.max(A[:,i] * delta);\n",
    "        idx[i] = np.argmax(A[:,i] * delta);\n",
    "    return vec.reshape(1,-1),idx.reshape(1,-1);\n",
    "\n",
    "\n",
    "# 状态空间 [1 2 3] 观测空间 [1，2](红 白)\n",
    "# 预测序列 p={红，白，红}\n",
    "# 维特比算法预测状态序列:\n",
    "def veterbi_cal(Pi,A,B,obs):\n",
    "    seq = obs;\n",
    "    times = len(seq);\n",
    "    #n为状态空间大小，m为观测空间大小\n",
    "    [n,m]=B.shape;\n",
    "    # delta初始化, 每1列代表1次观测,一共\n",
    "    # delta = [d(0),d(1),....d(times-1)]\n",
    "    # vaphi 类似\n",
    "    delta = np.zeros([n,times]);\n",
    "    vaphi = np.zeros([n,times],dtype=np.int64);\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        if(i == 0):\n",
    "            delta[:,i] = Pi * B[:,obs[i]];  \n",
    "            vaphi[:,i] = vaphi[:,i];\n",
    "        else:\n",
    "            # (1xn).T * (nx1) = nx1               \n",
    "            # 然后扩充到times列 nxtimes \n",
    "            vec,idx  = max_delta(delta[:,i-1],A);\n",
    "            delta[:,i] = vec * B[:,obs[i]]; \n",
    "            vaphi[:,i] = idx;\n",
    "    \n",
    "    state = [];\n",
    "    val = np.max(delta[:,i]);\n",
    "    # index是最后一个状态值的index\n",
    "    index = np.argmax(delta[:,i]);\n",
    "    state.insert(0,index);\n",
    "    # 反溯\n",
    "    # vaphi[:i]是类似[0;1;2]的向量，i当前反溯的最优路径的节点,vi是上一最优路径节点\n",
    "    for i in range(len(seq)-2,-1,-1):\n",
    "        index = vaphi[:,i+1][int(index)];\n",
    "        # 表头插入，表示反向\n",
    "        state.insert(0,int(index));\n",
    "    \n",
    "    return state;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 计算观测obs的概率\n",
    "# 有前向和后向两种实现方法,还可以综合使用，得出的结果都应该是一致的\n",
    "# 输入obs = [0 1 0] (红白红)\n",
    "def forward_method(Pi,A,B,obs):\n",
    "    times = len(obs);\n",
    "    #n为状态空间大小，m为观测空间大小\n",
    "    [n,m]=B.shape;\n",
    "    #alpha 初始化 alpha(i)全部保存下来\n",
    "    alpha = np.zeros([n,times]);\n",
    "    for i in range(times):\n",
    "        if (i == 0):\n",
    "            alpha[:,i] = Pi * B[:,obs[i]];\n",
    "        else:\n",
    "            alpha[:,i] = np.dot(A.T,alpha[:,i-1]) * B[:,obs[i]];\n",
    "    #final\n",
    "    p = sum(alpha[:,i]);\n",
    "    return p,alpha;\n",
    "\n",
    "def backward_method(Pi,A,B,obs):\n",
    "    times = len(obs);\n",
    "    #n为状态空间大小，m为观测空间大小\n",
    "    [n,m]=B.shape;\n",
    "    #后向倒序\n",
    "    beta = np.ones([n,times]);\n",
    "    for i in range(times-1,-1,-1):\n",
    "        if(i == times -1):\n",
    "            beta[:,i] = beta[:,i]; # keep 1\n",
    "        else:\n",
    "            beta[:,i] = np.dot(A,beta[:,i+1] * B[:,obs[i+1]]);\n",
    "    #final\n",
    "    p = sum(Pi * B[:,obs[i]] * beta[:,i]);\n",
    "\n",
    "    return p,beta;\n",
    "\n",
    "# 可以计算观测预测时刻t,某状态为qi的概率;\n",
    "# 可以计算观测预测时刻t,某状态为qi,时刻t+1,某状态为qj的联合概率;\n",
    "# 根据观测预测某个状态在T内出现的期望值..\n",
    "def p_cal(Pi,A,B,obs):\n",
    "\n",
    "    [p1,alpha] = forward_method(Pi,A,B,obs);\n",
    "    [p2,beta]  = backward_method(Pi,A,B,obs);\n",
    "\n",
    "    #666\n",
    "    p3 = sum (np.dot(A , B[:,obs[-1]] * beta[:,-1]) * alpha[:,-2]);\n",
    "\n",
    "    return p1,p2,p3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #   | 0.5 0.3 0.2 |    |0.5 0.5|\n",
    "    # A=| 0.3 0.5 0.2 | B= |0.4 0.6|\n",
    "    #   | 0.2 0.3 0.5 |    |0.7 0.3|\n",
    "    A=[[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]];\n",
    "    B=[[0.5,0.5],[0.4,0.6],[0.7,0.3]];\n",
    "    Pi=[0.2,0.4,0.4];\n",
    "    obs = [0,1,0];\n",
    "\n",
    "    A =np.array(A);\n",
    "    B=np.array(B);\n",
    "    Pi =np.array(Pi);\n",
    "    obs =np.array(obs);\n",
    "\n",
    "    print('observe seqence probability cal(fp):',forward_method(Pi,A,B,obs));\n",
    "    print('observe seqence probability cal(bp):',backward_method(Pi,A,B,obs));\n",
    "    print('observe seqence probability cal(total):',p_cal(Pi,A,B,obs));\n",
    "    print('veterbi cal:',veterbi_cal(Pi,A,B,obs));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化\n",
    "\n",
    "没有向量化，完全不能体现python的优势，下面看看概率计算中把前后，后向合并的一个公式:\n",
    "\n",
    "$P(O|\\lambda) = \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left( \\alpha_{t}(i)a_{ij}b_{j}(o_{t+1})\\beta_{t+1}(j) \\right) , t=1,2,3,...T-1$\n",
    "\n",
    "先不考虑i, 可以看出$\\beta$和$b$是向量点乘，而$a_{ij}$是与前面点乘是矩阵乘法的关系.\n",
    "这样i的累加通过矩阵乘法已经算完了，外围的j用向量点乘，再sum即可，即:\n",
    "```python\n",
    "    p3 = sum (np.dot(A , B[:,obs[-1]] * beta[:,-1]) * alpha[:,-2]);\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
