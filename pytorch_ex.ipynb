{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare(pytorch 0.3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2c9a63a670>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1.post3'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  0  0  0  0\n",
       " 0  1  0  0  0\n",
       " 0  0  1  0  0\n",
       " 0  0  0  1  0\n",
       " 0  0  0  0  1\n",
       "[torch.FloatTensor of size 5x5]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.eye(5,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(10,10)\n",
    "y=torch.randn(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "[torch.ByteTensor of size 10x10]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y == torch.add(x,y);\n",
    "res = torch.zeros(x.size())\n",
    "torch.add(x,y,out=res);\n",
    "res==x+y\n",
    "res==x.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.6614  0.2669  0.0617  0.6213 -0.4519 -0.1661 -1.5228  0.3817 -1.0276 -0.5631\n",
       "-0.3968 -0.6571 -1.6428  0.9803 -0.0421 -0.8206  0.3133 -1.1352  0.3773 -0.2824\n",
       "\n",
       "Columns 10 to 19 \n",
       "-0.8923 -0.0583 -0.1955 -0.9656  0.4224  0.2673 -0.4212 -0.5107 -1.5727 -0.1232\n",
       "-2.5667 -1.4303  0.5009  0.5438 -0.4057  1.1341 -1.1115  0.3501 -0.7703 -0.1473\n",
       "\n",
       "Columns 20 to 29 \n",
       " 3.5870 -1.8313  1.5987 -1.2770  0.3255 -0.4791  1.3790  2.5286  0.4107 -0.9880\n",
       " 0.6272  1.0935  0.0939  1.2381 -1.3459  0.5119 -0.6933 -0.1668 -0.9999 -1.6476\n",
       "\n",
       "Columns 30 to 39 \n",
       "-0.9081  0.5423  0.1103 -2.2590  0.6067 -0.1383  0.8310 -0.2477 -0.8029  0.2366\n",
       " 0.8098  0.0554  1.1340 -0.5326  0.6592 -1.5964 -0.3769 -3.1020 -0.0995 -0.7213\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.2857  0.6898 -0.6331  0.8795 -0.6842  0.4533  0.2912 -0.8317 -0.5525  0.6355\n",
       " 1.2708 -0.0020 -1.0952  0.6016  0.6984 -0.8005  1.5381  1.4673  1.5951 -1.5279\n",
       "[torch.FloatTensor of size 2x50]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshape\n",
    "x.view(2,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable wrap Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.6770  0.0650 -1.2248  1.4444 -1.0620 -1.4621 -2.4662  1.0500  0.1352 -0.8859\n",
       " 0.9859 -0.6249  0.2061 -1.0809  0.7395  0.8302  0.4450 -0.8635 -1.2244  1.0138\n",
       " 3.2531 -3.3037  2.3283 -1.4082 -0.3113  0.5638  1.8693  3.5604 -0.1882  0.6135\n",
       "-1.9815 -0.6750  0.7575 -2.3002  0.4292 -0.6383  1.6983 -0.5209 -1.2637  0.1375\n",
       " 0.7585  1.6948 -0.9202 -0.2824 -0.6566  1.0185  0.2797 -0.1611 -1.0454  2.1405\n",
       "-2.7232  0.9598 -2.5454  1.1540  0.0351 -1.7545  0.4047  0.2588 -0.3103 -0.7883\n",
       "-2.3743 -0.7142 -0.3110 -0.9179 -0.1730  1.3236 -1.3319  0.4991 -0.7603 -0.2716\n",
       " 2.0969  0.6984 -0.4162  2.3544 -1.9385  1.4208 -1.7725 -0.8419  0.4084 -3.4932\n",
       " 0.2443 -0.8539  0.6449 -1.1501  1.0428 -1.2112  0.4121 -1.9657 -0.2142 -0.7393\n",
       " 1.3199  0.4252 -1.9948  1.1329  1.1018  0.6516 -0.8800  0.2767  2.2915 -0.3983\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Variable\n",
    "\n",
    "v_x = autograd.Variable(x,requires_grad=True);\n",
    "v_y = autograd.Variable(y,requires_grad=True);\n",
    "v_z = v_x + v_y\n",
    "\n",
    "v_z.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward1 at 0x7f2c8c7442b0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f =v_z.grad_fn\n",
    "grad_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backwards gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = v_z.sum()\n",
    "# start calculate using bp\n",
    "s .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dx \n",
    "v_x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy\n",
    "v_y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dz? None\n",
    "type(v_z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-0.2280  0.3141  0.0029 -0.3590 -0.0937\n",
      "-0.4037 -0.2317 -0.3168  0.3059 -0.0940\n",
      "-0.0773  0.2143  0.1820 -0.2951  0.1648\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1900\n",
      "-0.3556\n",
      " 0.3888\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      " 0.6196  0.1353  0.5635\n",
      " 1.1082 -0.3728  1.1576\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
    "# init by random\n",
    "print(lin.weight)\n",
    "print(lin.bias)\n",
    "#data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
    "data = autograd.Variable(torch.randn(2, 5))\n",
    "print(lin(data))  # yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = lin(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding_idx的意思是，如果原indice为0，则得到的embedding向量也为0\n",
    "\n",
    "6 表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = nn.Embedding(6, 3, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100表示总的embedding indice的大小，词典单词的总个数之类的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(100, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入时，可以按batch_num x indices来输入 , batch_num表示batch_num 个batch, indices表示一个batch的大小\n",
    "至于RNN里有多少个loop,跟这个embedding一点关系也没有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记住,torch里的nn.Embedding也好，nn.Linear也好,都是定义了一个系数矩阵;\n",
    "如Linear:\n",
    "    nn.Linear(10,20) 相当于将输入x的10维度通过linear layer变为20层:\n",
    "```Y=W*X + b ```\n",
    "\n",
    "实际输入的X,可以是10x1, 也可以是`10 x batch_num x batch_size`这样的形式.\n",
    "\n",
    "最后的输出也具有形式`batch_num x batch_size`而已:\n",
    "```W(20x10) * X(10x1000x10) = Y (20x1000x10)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-97e30d622f60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.randint(0,12,(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " inputs = autograd.Variable(torch.LongTensor(np.random.randint(0,12,(100,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " inputs = autograd.Variable(torch.LongTensor([[1,1,0,0,2,0,5,2,1,1],[10,10,1,1,1,2,3,4,1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX0 Liner regression using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用torch计算梯度，最底层Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = 30000\n",
    "a = 10\n",
    "b= 3\n",
    "c = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=np.linspace(-1,1,lens) \n",
    "y= a* np.square(o) + b* o + c\n",
    "x = o+  np.random.randn(lens,)*0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2c8e0b35c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's predict a,b,c\n",
    "y = ax^2 + bx +c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=torch.from_numpy(x).type(torch.FloatTensor)\n",
    "Y_t=torch.from_numpy(y).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=autograd.Variable(X_t)\n",
    "Y=autograd.Variable(Y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=autograd.Variable(torch.randn(1),requires_grad=True)\n",
    "b=autograd.Variable(torch.randn(1),requires_grad=True)\n",
    "c=autograd.Variable(torch.randn(1),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.3790\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linmodel(a,b,c,x): return a*(x**2) + b*x +c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred,y): return ((y_pred-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(a,b,c,x,y): return mse(linmodel(a,b,c,x),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =0.05\n",
    "epochs=400\n",
    "batches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  36.55488967895508\n",
      "loss  4.538230895996094\n",
      "loss  2.7963781356811523\n",
      "loss  2.0035440921783447\n",
      "loss  1.5090769529342651\n",
      "loss  1.1730295419692993\n",
      "loss  0.9368081092834473\n",
      "loss  0.7686601281166077\n",
      "loss  0.6484246253967285\n",
      "loss  0.5623102188110352\n",
      "loss  0.5005981922149658\n",
      "loss  0.4563653767108917\n",
      "loss  0.4246579706668854\n",
      "loss  0.40192911028862\n",
      "loss  0.3856358528137207\n",
      "loss  0.37395575642585754\n",
      "loss  0.36558303236961365\n",
      "loss  0.359580934047699\n",
      "loss  0.35527849197387695\n",
      "loss  0.35219424962997437\n"
     ]
    }
   ],
   "source": [
    "a=autograd.Variable(torch.randn(1),requires_grad=True)\n",
    "b=autograd.Variable(torch.randn(1),requires_grad=True)\n",
    "c=autograd.Variable(torch.randn(1),requires_grad=True)\n",
    "for i in range(epochs):\n",
    "    \n",
    "    loss = mse_loss(a,b,c,X,Y)\n",
    "    if i % 20 ==0: print ('loss ', loss.data[0])\n",
    "    loss.backward()\n",
    "    \n",
    "    a.data   -= lr * a.grad.data\n",
    "    b.data   -= lr * b.grad.data\n",
    "    c.data   -= lr * c.grad.data\n",
    "    \n",
    "    a.grad.data.zero_();\n",
    "    b.grad.data.zero_();\n",
    "    c.grad.data.zero_();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用nn.Linear  + optim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Model Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "y = a*x^2  + bx +c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class L2Model(nn.Module):\n",
    "    def __init__(self,n_x):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(n_x,1)\n",
    "    # input should be [ x  x^2] x  batch_size   for each batch\n",
    "    def forward(self,*X): \n",
    "        bs = X[0].size(0)\n",
    "        #X here is a tuple \n",
    "        return self.lin(torch.t(torch.stack((X[0],X[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lm = L2Model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.6309 -0.5963\n",
       " [torch.FloatTensor of size 1x2], Parameter containing:\n",
       "  0.2659\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in lm.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "model需要什么样的输入?Variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.0038e+00\n",
       "-9.8713e-01\n",
       "-9.8563e-01\n",
       "     ⋮     \n",
       " 9.8686e-01\n",
       " 9.5338e-01\n",
       " 1.0648e+00\n",
       "[torch.FloatTensor of size 30000]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.0038e+00  1.0076e+00\n",
       "-9.8713e-01  9.7443e-01\n",
       "-9.8563e-01  9.7146e-01\n",
       "           ⋮            \n",
       " 9.8686e-01  9.7390e-01\n",
       " 9.5338e-01  9.0893e-01\n",
       " 1.0648e+00  1.1337e+00\n",
       "[torch.FloatTensor of size 30000x2]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stack = torch.stack((X,X**2))\n",
    "X_stack = torch.t(X_stack)\n",
    "X_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-9.6821e-01\n",
       "-9.3793e-01\n",
       "-9.3521e-01\n",
       "     ⋮      \n",
       " 3.0778e-01\n",
       " 3.2539e-01\n",
       " 2.6163e-01\n",
       "[torch.FloatTensor of size 30000x1]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = lm(X,X**2)\n",
    "Y_pred\n",
    "# full-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "用optim 来梯度下降, lm算前向，backward bp算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### full-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss:  47.060848236083984\n",
      "mse loss:  0.37773704528808594\n",
      "mse loss:  0.3445877730846405\n",
      "mse loss:  0.3443857431411743\n",
      "mse loss:  0.3443855047225952\n",
      "mse loss:  0.3443852663040161\n",
      "mse loss:  0.3443852961063385\n",
      "mse loss:  0.3443852961063385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  2.9648  9.5871\n",
       " [torch.FloatTensor of size 1x2], Parameter containing:\n",
       "  2.1049\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = L2Model(2)\n",
    "optimizer = optim.SGD(lm.parameters(), lr=0.3)\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    Y_pred = lm(X,X**2)\n",
    "    loss = F.mse_loss(Y_pred,Y)\n",
    "    if e % 50 ==0: print('mse loss: ',loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lm.zero_grad()\n",
    "\n",
    "[p for p in lm.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 58)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=512\n",
    "range(lens // bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 512\n",
    "left = lens - bs* (lens//bs)\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9,10]\n",
    "left =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 10]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[10-left:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bs = 512\n",
    "# 把数据分成batches.\n",
    "X2=X**2\n",
    "bs_data = [ (X[j*bs:(j+1)*bs],X2[j*bs:(j+1)*bs],Y[j*bs:(j+1)*bs]) for j in range(lens // bs) ]\n",
    "# 剩余不足512整数倍的数据 需要再添加一个batch\n",
    "left = lens - bs* (lens//bs)\n",
    "bs_data.append(\n",
    "    (\n",
    "        X[lens-left:],\n",
    "        X2[lens-left:],\n",
    "        Y[lens-left:]\n",
    "        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def proc(data):\n",
    "    X0=data[0]\n",
    "    X1=data[1]\n",
    "    Y=data[2]        \n",
    "    Y_pred = lm(X0,X1)\n",
    "    loss = F.mse_loss(Y_pred,Y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lm.zero_grad()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  0 batchs:  0 mse_loss:  91.04118347167969\n",
      "epochs:  0 batchs:  20 mse_loss:  0.016344910487532616\n",
      "epochs:  0 batchs:  40 mse_loss:  6.791793346405029\n",
      "epochs:  5 batchs:  0 mse_loss:  10.354704856872559\n",
      "epochs:  5 batchs:  20 mse_loss:  1.0935184955596924\n",
      "epochs:  5 batchs:  40 mse_loss:  0.45009323954582214\n",
      "epochs:  10 batchs:  0 mse_loss:  2.6887059211730957\n",
      "epochs:  10 batchs:  20 mse_loss:  0.21073123812675476\n",
      "epochs:  10 batchs:  40 mse_loss:  0.30528363585472107\n",
      "epochs:  15 batchs:  0 mse_loss:  1.1499251127243042\n",
      "epochs:  15 batchs:  20 mse_loss:  0.07216770201921463\n",
      "epochs:  15 batchs:  40 mse_loss:  0.28250324726104736\n",
      "epochs:  20 batchs:  0 mse_loss:  0.8714697957038879\n",
      "epochs:  20 batchs:  20 mse_loss:  0.04497392848134041\n",
      "epochs:  20 batchs:  40 mse_loss:  0.2813211977481842\n",
      "epochs:  25 batchs:  0 mse_loss:  0.8082141876220703\n",
      "epochs:  25 batchs:  20 mse_loss:  0.038116034120321274\n",
      "epochs:  25 batchs:  40 mse_loss:  0.2818431258201599\n",
      "epochs:  30 batchs:  0 mse_loss:  0.7902522087097168\n",
      "epochs:  30 batchs:  20 mse_loss:  0.03602716699242592\n",
      "epochs:  30 batchs:  40 mse_loss:  0.28215375542640686\n",
      "epochs:  35 batchs:  0 mse_loss:  0.7844122052192688\n",
      "epochs:  35 batchs:  20 mse_loss:  0.035324711352586746\n",
      "epochs:  35 batchs:  40 mse_loss:  0.28228163719177246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  3.0556  9.5580\n",
       " [torch.FloatTensor of size 1x2], Parameter containing:\n",
       "  2.1228\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = L2Model(2)\n",
    "\n",
    "optimizer = optim.SGD(lm.parameters(), lr=0.02)\n",
    "\n",
    "eps = 40\n",
    "\n",
    "for e in range(eps):\n",
    "    for i,data in enumerate(bs_data):\n",
    "        loss = proc(data)\n",
    "        if (e % 5 == 0 ) and (i % 20 == 0): print('epochs: ',e,'batchs: ',i,'mse_loss: ',loss.data[0])\n",
    "    \n",
    "\n",
    "[p for p in lm.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### 换一个optim试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  0 batchs:  0 mse_loss:  77.9782943725586\n",
      "epochs:  0 batchs:  20 mse_loss:  1.4364030361175537\n",
      "epochs:  0 batchs:  40 mse_loss:  14.293922424316406\n",
      "epochs:  5 batchs:  0 mse_loss:  33.50813293457031\n",
      "epochs:  5 batchs:  20 mse_loss:  0.5396761298179626\n",
      "epochs:  5 batchs:  40 mse_loss:  1.8727866411209106\n",
      "epochs:  10 batchs:  0 mse_loss:  17.312225341796875\n",
      "epochs:  10 batchs:  20 mse_loss:  2.8354110717773438\n",
      "epochs:  10 batchs:  40 mse_loss:  0.029867982491850853\n",
      "epochs:  15 batchs:  0 mse_loss:  11.520794868469238\n",
      "epochs:  15 batchs:  20 mse_loss:  3.9585063457489014\n",
      "epochs:  15 batchs:  40 mse_loss:  0.5469508171081543\n",
      "epochs:  20 batchs:  0 mse_loss:  9.417847633361816\n",
      "epochs:  20 batchs:  20 mse_loss:  3.772393226623535\n",
      "epochs:  20 batchs:  40 mse_loss:  1.0282247066497803\n",
      "epochs:  25 batchs:  0 mse_loss:  8.335884094238281\n",
      "epochs:  25 batchs:  20 mse_loss:  3.0648369789123535\n",
      "epochs:  25 batchs:  40 mse_loss:  1.177186131477356\n",
      "epochs:  30 batchs:  0 mse_loss:  7.317866325378418\n",
      "epochs:  30 batchs:  20 mse_loss:  2.3192577362060547\n",
      "epochs:  30 batchs:  40 mse_loss:  1.1235145330429077\n",
      "epochs:  35 batchs:  0 mse_loss:  6.155473232269287\n",
      "epochs:  35 batchs:  20 mse_loss:  1.6944282054901123\n",
      "epochs:  35 batchs:  40 mse_loss:  0.9816274642944336\n",
      "epochs:  40 batchs:  0 mse_loss:  4.932776927947998\n",
      "epochs:  40 batchs:  20 mse_loss:  1.2089056968688965\n",
      "epochs:  40 batchs:  40 mse_loss:  0.8153459429740906\n",
      "epochs:  45 batchs:  0 mse_loss:  3.7898178100585938\n",
      "epochs:  45 batchs:  20 mse_loss:  0.8430916666984558\n",
      "epochs:  45 batchs:  40 mse_loss:  0.6588000059127808\n",
      "epochs:  50 batchs:  0 mse_loss:  2.8262948989868164\n",
      "epochs:  50 batchs:  20 mse_loss:  0.5734890103340149\n",
      "epochs:  50 batchs:  40 mse_loss:  0.528980553150177\n",
      "epochs:  55 batchs:  0 mse_loss:  2.0826923847198486\n",
      "epochs:  55 batchs:  20 mse_loss:  0.38009849190711975\n",
      "epochs:  55 batchs:  40 mse_loss:  0.43161123991012573\n",
      "epochs:  60 batchs:  0 mse_loss:  1.5527557134628296\n",
      "epochs:  60 batchs:  20 mse_loss:  0.24637287855148315\n",
      "epochs:  60 batchs:  40 mse_loss:  0.36509546637535095\n",
      "epochs:  65 batchs:  0 mse_loss:  1.202951192855835\n",
      "epochs:  65 batchs:  20 mse_loss:  0.15812960267066956\n",
      "epochs:  65 batchs:  40 mse_loss:  0.323855459690094\n",
      "epochs:  70 batchs:  0 mse_loss:  0.9892903566360474\n",
      "epochs:  70 batchs:  20 mse_loss:  0.1030246838927269\n",
      "epochs:  70 batchs:  40 mse_loss:  0.30094555020332336\n",
      "epochs:  75 batchs:  0 mse_loss:  0.8688058853149414\n",
      "epochs:  75 batchs:  20 mse_loss:  0.07061976939439774\n",
      "epochs:  75 batchs:  40 mse_loss:  0.28982529044151306\n",
      "epochs:  80 batchs:  0 mse_loss:  0.8061395883560181\n",
      "epochs:  80 batchs:  20 mse_loss:  0.05266750976443291\n",
      "epochs:  80 batchs:  40 mse_loss:  0.2853321433067322\n",
      "epochs:  85 batchs:  0 mse_loss:  0.7759890556335449\n",
      "epochs:  85 batchs:  20 mse_loss:  0.04326211288571358\n",
      "epochs:  85 batchs:  40 mse_loss:  0.2840050756931305\n",
      "epochs:  90 batchs:  0 mse_loss:  0.7625846266746521\n",
      "epochs:  90 batchs:  20 mse_loss:  0.03861228749155998\n",
      "epochs:  90 batchs:  40 mse_loss:  0.2839046120643616\n",
      "epochs:  95 batchs:  0 mse_loss:  0.7573392987251282\n",
      "epochs:  95 batchs:  20 mse_loss:  0.03650865703821182\n",
      "epochs:  95 batchs:  40 mse_loss:  0.2841581702232361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  2.9919  9.6088\n",
       " [torch.FloatTensor of size 1x2], Parameter containing:\n",
       "  2.0824\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = L2Model(2)\n",
    "\n",
    "optimizer = optim.Adam(lm.parameters(),lr=0.01)\n",
    "\n",
    "eps = 100\n",
    "\n",
    "for e in range(eps):\n",
    "    for i,data in enumerate(bs_data):\n",
    "        loss = proc(data)\n",
    "        if (e % 5 == 0 ) and (i % 20 == 0): print('epochs: ',e,'batchs: ',i,'mse_loss: ',loss.data[0])\n",
    "    \n",
    "\n",
    "[p for p in lm.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 1 : Bag of Words using logSoftmax ex\n",
    "\n",
    "用pytorch解决一个多分类的经典问题\n",
    "\n",
    "model ---> forward计算\n",
    "\n",
    "lossfunciton 反向bp ,更新参数 ,\n",
    "loss.backword()\n",
    "optim.step()\n",
    "\n",
    "再用 model预测新样本 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0475 -0.1450  0.1674 -0.0761  0.1181  0.0058 -0.0153 -0.0063  0.0333  0.0924\n",
      "-0.0449  0.1427  0.1553  0.1855 -0.0398 -0.1524  0.1931 -0.0418 -0.0807  0.0478\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0315  0.0598 -0.1764  0.1429  0.1710  0.1621  0.1450 -0.1415 -0.0727  0.1729\n",
      "-0.1371  0.1289  0.1229 -0.1556 -0.1611 -0.0172  0.0824 -0.0057 -0.0994  0.0045\n",
      "\n",
      "Columns 20 to 25 \n",
      "-0.1494  0.1779 -0.1542 -0.1381  0.0959 -0.1409\n",
      "-0.1843 -0.1386 -0.1306  0.1615  0.1729 -0.0666\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      "  0.8796\n",
      "  8.7463\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "bow_vec \n",
      "\n",
      "Columns 0 to 12 \n",
      "    1     1     1     1     1     1     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.8021 -0.5949\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea is\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "# 'SPAINIS' 'ENGLISH''SPAINIS' 'ENGLISH''SPAINIS' 'ENGLISH'--->'0,1,0,1'\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the Pytorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "print('bow_vec',bow_vector)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0731 -2.6527\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-3.5073 -0.0304\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.6999\n",
      "-0.6981\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "for instance ['Yo', 'creo', 'que', 'si']\n",
      "instance log_prob:\n",
      " Variable containing:\n",
      "-0.0657 -2.7551\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "instance prob:\n",
      " Variable containing:\n",
      " 0.9364  0.0636\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "for instance ['it', 'is', 'lost', 'on', 'me']\n",
      "instance log_prob:\n",
      " Variable containing:\n",
      "-3.6621 -0.0260\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "instance prob:\n",
      " Variable containing:\n",
      " 0.0257  0.9743\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.7252\n",
      "-0.7234\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data before we train, just to see a before-and-after\n",
    "for instance, label in test_data:\n",
    "    # 相当于特征化一个样本 \n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    #模型跑一次，like predict\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Variable as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print('for instance', instance)\n",
    "    probs = torch.exp(log_probs)\n",
    "    print('instance log_prob:\\n',log_probs)\n",
    "    print('instance prob:\\n',probs)\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 得到的参数后的解释\n",
    "\n",
    "对于`creo`这个词:\n",
    "\n",
    "Variable containing:\n",
    " 0.7252\n",
    "-0.7234\n",
    "[torch.FloatTensor of size 2]\n",
    "\n",
    "0.72的权重偏向Spanish -0.72权重偏向English 这就是物理意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Ex 2 :LeNet For Image Classification ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3 : Fastai movilen lesson 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 6  6\n",
       " 4  4\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor( [ [2,3],[1,4] ])\n",
    "b= torch.Tensor([ [3,2],[4,1] ])\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 10\n",
       " 10\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class dotMultipy(nn.Module):\n",
    "    def forward(self,a,b): return  sum(a*b)\n",
    "    \n",
    "model = dotMultipy()\n",
    "model(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House price prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
