{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id3 分类树\n",
    "id3以信息熵作为特征分裂的标准。\n",
    "\n",
    "g(D,A) = H(D) - H(D|A);\n",
    "\n",
    "`信息熵`定义:\n",
    "\n",
    "$H(D)=\\sum_{i=1}^{k}p(D=d_{i})*log(p(D=d_{i}))=\\sum_{i=1}^{k}p_{i}*log(p_{i})$\n",
    "\n",
    "注意区分`单个条件下的熵`\n",
    "\n",
    "$H(D|A=a) = \\sum_{i=1}^{k}p(d_{i}|a)*log(p(d_{i}|a))$\n",
    "\n",
    "与`条件熵`区别,后者是前者的期望值\n",
    "\n",
    "<span>$\\begin{align*}\n",
    "H(D|A) = E[H(D|a)]=\\sum_{i=1}^{k} p(a)*H(D|a)\n",
    "\\\\\n",
    "=\\sum_{i=1}^{k} p(a_{i})*\\sum_{j=1}^{n}p(d_{j}|a_{i})*log(p(d_{j}|a_{i}))\n",
    "\\\\\n",
    "=\\sum_{i=1}^{k}\\sum_{j=1}^{n}p(a_{i},d_{j})*log(p(d_{j}|a_{i}))\n",
    "\\end{align*}$</span>\n",
    "\n",
    "\n",
    "###  C4.5 分类树\n",
    "\n",
    "c4.5 以信息熵比率作为特征分裂标准.\n",
    "\n",
    "gg(D,A) = g(D,A)  / H(D);\n",
    "\n",
    "### CART 回归树与分类树\n",
    "\n",
    "![2018-05-04-16-26-57](http://ozt5ysx10.bkt.clouddn.com/2018-05-04-16-26-57.png)\n",
    "\n",
    "可以知道,最耗时的一个步骤就是确定这个最佳分割点.\n",
    "\n",
    "### 剪枝算法\n",
    "\n",
    "### boost \n",
    "\n",
    "boosting的思想是用`加权的多个弱分类器之和来表示最终的目标分类器`.\n",
    "\n",
    "#### adaboost\n",
    "adaboost是一种boost算法.\n",
    "\n",
    "\n",
    "#### boost tree 提升树\n",
    "\n",
    "当弱分类器用的是`cart回归树/分类树`时，这样的boost算法叫做提升树\n",
    "\n",
    "提升回归树的思想是，训练第一个回归树后，计算样本空间里每一个样本的回归树预测值与原样本的残差，再对残差训练回归树，，目标分类器不断的叠加回归树，最后残差收敛掉.\n",
    "\n",
    "这个`残差`实际上是损失函数用平方误差函数时，推导出来的.\n",
    "\n",
    "算法过程是:\n",
    "\n",
    "<span>$\\begin{align*}\n",
    "f_{0}(x)=0;\n",
    "\\\\\n",
    "f_{m}(x)=f_{m-1}(x)+T(x,\\Theta_{m}) , \\ m=1,2,3,..M\n",
    "\\\\\n",
    "f_{M}(x)=\\sum_{i=1}^{M}T(x,\\Theta_{i})\n",
    "\\end{align*}$</span>\n",
    "\n",
    "其中$\\Theta_{m}$是第m颗回归树的参数{c1,c2....cn,R1,R2,...Rn};\n",
    "\n",
    "第m步时，给定第m-1步的模型$f_{m-1}(x)$,求解:\n",
    "\n",
    "$\\Theta_{m} = arg\\ \\underset{\\Theta_{m}}{min}\\sum_{i=1}^{N}(L(y_{i},f_{m-1}(x)+T(x,\\Theta_{m}))$\n",
    "\n",
    "当L损失函数是`平方误差`时，可以得到:\n",
    "\n",
    "$T(x,\\Theta_{m})=y-f_{m-1}(x)$\n",
    "\n",
    "所以用残差训练得到的树就是第m步需要的树.\n",
    "\n",
    "#### GBDT 梯度提升树\n",
    "\n",
    "上面提到损失函数是平方误差函数时，比较好计算。（指数函数也好算，adaboost）如果一般的损失函数，就需要用`梯度提升`\n",
    "来进行优化。\n",
    "\n",
    "方法是差不多的，不过是用`损失函数的负梯度`作为上面说的残差值的近似值，拟合回归树，求法为:\n",
    "\n",
    "$r_{mi}=-\\left [ \\frac{\\partial L(y_{i},f(x_i))}{\\partial f(x_i)} \\right ]_{f(x)=f_{m-1}(x)}$\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "比GBDT更正则化,正方便计算,另外除了用回归树，也支持线性分类器.\n",
    "\n",
    "[参考](https://www.zhihu.com/question/41354392)\n",
    "\n",
    "\n",
    "回顾下回归算法的步步升级过程:\n",
    "\n",
    "* 线性回归;\n",
    "* CART回归树回归;\n",
    "* boosting 回归树;\n",
    "* GBDT\n",
    "* xgboost\n",
    "\n",
    "\n",
    "### bagging + 决策树 = 随机森林\n",
    "\n",
    "\n",
    "### ensemble\n",
    "\n",
    "狭义上，boosting 和 bagging都是ensemble\n",
    "\n",
    "广义上, 集万家之所长，多个模型(LR Boost GBDT all together)一起, 再加权平均.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
